{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17256bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import math\n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b13f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: torch.cos(10*torch.pi*torch.abs(x)**(1.4))+0.5*torch.cos(6*torch.pi*torch.abs(x)**(1.6))\n",
    "# func = lambda x: torch.sin(5 * np.pi * x)  # Simpler function for testing\n",
    "# Quick integration test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d1f75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MMNN_BNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 ranks=[1] + [16]*5 + [1], \n",
    "                 widths=[366]*6,\n",
    "                 device=\"cpu\",\n",
    "                 prior_var=0.1,\n",
    "                 ResNet=False):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        MMNN_BNN: Bayesian Neural Network using MMNN structure\n",
    "        \n",
    "        Args:\n",
    "            ranks: Output dimensions per layer [input_dim, ..., output_dim]\n",
    "            widths: Width (number of basis functions) per layer\n",
    "            device: Computing device\n",
    "            prior_var: Prior variance for BNN layers (σ²/n_in scaling)\n",
    "            ResNet: Whether to use residual connections\n",
    "        \"\"\"\n",
    "        self.ranks = ranks\n",
    "        self.widths = widths\n",
    "        self.ResNet = ResNet\n",
    "        self.depth = len(widths)\n",
    "        self.device = device\n",
    "        self.prior_var = prior_var\n",
    "        \n",
    "        # Build layer sizes: [input, width1, rank1, width2, rank2, ..., output]\n",
    "        fc_sizes = [ranks[0]]\n",
    "        for j in range(self.depth):\n",
    "            fc_sizes += [widths[j], ranks[j+1]]\n",
    "        \n",
    "        # Initialize layers\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.bnn_params = nn.ParameterDict()  # BNN variance parameters\n",
    "        \n",
    "        for j in range(len(fc_sizes)-1):\n",
    "            # print(f\"Layer {j}: {fc_sizes[j]} -> {fc_sizes[j+1]}\")\n",
    "            fc = nn.Linear(fc_sizes[j], fc_sizes[j+1], device=device)\n",
    "            self.fcs.append(fc)\n",
    "            \n",
    "            # For W,b layers (even indices): Add BNN treatment\n",
    "            if j % 2 == 0:\n",
    "                # Freeze the mean parameters (W, b)\n",
    "                fc.weight.requires_grad = False\n",
    "                fc.bias.requires_grad = False\n",
    "                \n",
    "                # Store fixed means\n",
    "                self.register_buffer(f'W_mean_{j}', fc.weight.clone())\n",
    "                self.register_buffer(f'b_mean_{j}', fc.bias.clone())\n",
    "                \n",
    "                # Initialize variance parameters (log-parameterization for stability)\n",
    "                # Depth-dependent prior variance\n",
    "                layer_depth = j // 2  # Convert to layer index\n",
    "                layer_prior_var = self.get_layer_prior_var(layer_depth, self.prior_var)\n",
    "                n_in = fc_sizes[j]\n",
    "                init_log_var = math.log(layer_prior_var / n_in)  # σ²/n_in scaling\n",
    "\n",
    "                \n",
    "                \n",
    "                self.bnn_params[f'W_log_var_{j}'] = nn.Parameter(\n",
    "                    torch.full_like(fc.weight, init_log_var)\n",
    "                )\n",
    "                self.bnn_params[f'b_log_var_{j}'] = nn.Parameter(\n",
    "                    torch.full_like(fc.bias, init_log_var)\n",
    "                )\n",
    "    def get_layer_prior_var(self, layer_depth, base_var=0.1):\n",
    "        \"\"\"\n",
    "        Depth-dependent prior variance\n",
    "        \n",
    "        Args:\n",
    "            layer_depth: 0 for first layer, increasing with depth\n",
    "            base_var: Base variance for last layer\n",
    "        \"\"\"\n",
    "        # Exponential decay with depth (deeper = higher variance allowed)\n",
    "        depth_factor = 10 ** (-2 * (self.depth - 1 - layer_depth) / (self.depth - 1))\n",
    "        return base_var * depth_factor\n",
    "    def sample_bnn_params(self):\n",
    "        \"\"\"Sample W,b parameters from variational posterior\"\"\"\n",
    "        sampled_params = {}\n",
    "        \n",
    "        for name, param in self.bnn_params.items():\n",
    "            if 'W_log_var' in name:\n",
    "                layer_idx = name.split('_')[-1]\n",
    "                \n",
    "                # Get mean and variance\n",
    "                W_mean = getattr(self, f'W_mean_{layer_idx}')\n",
    "                W_var = torch.exp(param)  # Convert from log-variance\n",
    "                \n",
    "                # Sample using reparameterization trick\n",
    "                eps = torch.randn_like(W_mean) # Generate i.i.d. samples with shape (n_in, n_out)\n",
    "                W_sample = W_mean + torch.sqrt(W_var) * eps # Generate \n",
    "                sampled_params[f'W_{layer_idx}'] = W_sample\n",
    "                \n",
    "            elif 'b_log_var' in name:\n",
    "                layer_idx = name.split('_')[-1]\n",
    "                \n",
    "                b_mean = getattr(self, f'b_mean_{layer_idx}') # Generate i.i.d. samples with shape (n_in, 1)\n",
    "                b_var = torch.exp(param)\n",
    "                \n",
    "                eps = torch.randn_like(b_mean)\n",
    "                b_sample = b_mean + torch.sqrt(b_var) * eps\n",
    "                sampled_params[f'b_{layer_idx}'] = b_sample\n",
    "        \n",
    "        return sampled_params\n",
    "    \n",
    "    def forward(self, x, n_samples=1):\n",
    "        \"\"\"\n",
    "        Forward pass with Monte Carlo sampling\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            n_samples: Number of MC samples for inference\n",
    "        \n",
    "        Returns:\n",
    "            output: Mean prediction (if n_samples=1) or samples\n",
    "        \"\"\"\n",
    "        if n_samples == 1:\n",
    "            return self._forward_single(x)\n",
    "        else:\n",
    "            return self._forward_mc(x, n_samples)\n",
    "    \n",
    "    def _forward_single(self, x):\n",
    "        \"\"\"Single forward pass for training\"\"\"\n",
    "        sampled_params = self.sample_bnn_params()\n",
    "        \n",
    "        for j in range(self.depth):\n",
    "            if self.ResNet and 0 < j < self.depth-1:\n",
    "                x_id = x.clone()\n",
    "            \n",
    "            # BNN layer (W,b with uncertainty)\n",
    "            layer_idx = 2 * j\n",
    "            if f'W_{layer_idx}' in sampled_params:\n",
    "                W = sampled_params[f'W_{layer_idx}']\n",
    "                b = sampled_params[f'b_{layer_idx}']\n",
    "                # print(W.t().shape, b.shape,x.shape)\n",
    "                x = F.linear(x, W, b)\n",
    "            else:\n",
    "                x = self.fcs[layer_idx](x)\n",
    "            \n",
    "            x = torch.relu(x)\n",
    "            \n",
    "            # Deterministic A layer\n",
    "            x = self.fcs[layer_idx + 1](x)\n",
    "            \n",
    "            # ResNet connection\n",
    "            if self.ResNet and 0 < j < self.depth-1:\n",
    "                n = min(x.shape[1], x_id.shape[1])\n",
    "                x[:, :n] = x[:, :n] + x_id[:, :n]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _forward_mc(self, x, n_samples):\n",
    "        \"\"\"Multiple forward passes for uncertainty estimation\"\"\"\n",
    "        outputs = []\n",
    "        for _ in range(n_samples):\n",
    "            outputs.append(self._forward_single(x))\n",
    "        return torch.stack(outputs, dim=0)  # [n_samples, batch_size, output_dim]\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        \"\"\"Compute KL divergence between posterior and prior\"\"\"\n",
    "        kl_div = 0.0\n",
    "        \n",
    "        for name, log_var_param in self.bnn_params.items():\n",
    "            if 'W_log_var' in name:\n",
    "                layer_idx = name.split('_')[-1]\n",
    "                layer_idx_int = int(layer_idx)\n",
    "                n_in = self.fcs[layer_idx_int].in_features\n",
    "                \n",
    "                # Prior: N(W_mean, σ²/n_in * I)\n",
    "                prior_log_var = math.log(self.prior_var / n_in)\n",
    "                \n",
    "            elif 'b_log_var' in name:\n",
    "                # Prior: N(b_mean, σ² * I) \n",
    "                prior_log_var = math.log(self.prior_var)\n",
    "            \n",
    "            # KL[N(μ, σ²) || N(μ, σ_p²)] = 1/2 * [log(σ_p²/σ²) + σ²/σ_p² - 1]\n",
    "            posterior_var = torch.exp(log_var_param)\n",
    "            prior_var = math.exp(prior_log_var)\n",
    "            \n",
    "            kl_layer = 0.5 * (\n",
    "                prior_log_var - log_var_param + \n",
    "                posterior_var / prior_var - 1.0\n",
    "            )\n",
    "            kl_div += kl_layer.sum()\n",
    "        \n",
    "        return kl_div\n",
    "    \n",
    "    def elbo_loss(self, x, y, n_samples=1, beta=1.0):\n",
    "        \"\"\"\n",
    "        Compute ELBO loss for training\n",
    "        \n",
    "        Args:\n",
    "            x, y: Input and target\n",
    "            n_samples: MC samples for likelihood estimation\n",
    "            beta: KL weighting (β-VAE style)\n",
    "        \"\"\"\n",
    "        # Likelihood term: E_q[log p(y|x,θ)]\n",
    "        if n_samples == 1:\n",
    "            y_pred = self._forward_single(x)\n",
    "            log_likelihood = -0.5 * F.mse_loss(y_pred, y, reduction='sum')\n",
    "        else:\n",
    "            y_samples = self._forward_mc(x, n_samples)  # [n_samples, batch, output]\n",
    "            log_likelihood = 0.0\n",
    "            for i in range(n_samples):\n",
    "                log_likelihood += -0.5 * F.mse_loss(y_samples[i], y, reduction='sum')\n",
    "            log_likelihood /= n_samples\n",
    "        \n",
    "        # KL term: KL[q(θ)||p(θ)]\n",
    "        kl_div = self.kl_divergence()\n",
    "        \n",
    "        # ELBO = E[log p(y|x,θ)] - β * KL[q(θ)||p(θ)]\n",
    "        elbo = log_likelihood - beta * kl_div\n",
    "        \n",
    "        # Return negative ELBO for minimization\n",
    "        return -elbo, log_likelihood, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fe7b880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mmnn_bnn(model, train_loader, val_loader=None, \n",
    "                   epochs=1000, lr=1e-3, beta_schedule=None, \n",
    "                   n_mc_samples=1, device='cpu'):\n",
    "    \"\"\"\n",
    "    Training scheme for MMNN_BNN\n",
    "    \n",
    "    Args:\n",
    "        model: MMNN_BNN instance\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader (optional)\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        beta_schedule: Function for KL annealing (epoch -> beta)\n",
    "        n_mc_samples: MC samples during training (1 for efficiency)\n",
    "        device: Computing device\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optimizer - separate learning rates for different parameter types\n",
    "    bnn_params = list(model.bnn_params.parameters())\n",
    "    bnn_param_ids = {id(p) for p in bnn_params}\n",
    "    mlp_params = [p for p in model.parameters() if p.requires_grad and id(p) not in bnn_param_ids]\n",
    "    \n",
    "    optimizer = optim.Adam([\n",
    "        {'params': bnn_params, 'lr': lr, 'name': 'bnn'},\n",
    "        {'params': mlp_params, 'lr': lr , 'name': 'mlp'}\n",
    "    ])\n",
    "    \n",
    "    # scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)  \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=400, gamma=0.9)\n",
    "    \n",
    "    # Default beta schedule: gradual annealing from 0 to 1\n",
    "    if beta_schedule is None:\n",
    "        beta_schedule = lambda epoch: min(1.0, epoch / (epochs /200))\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_likelihood': [], 'train_kl': [],\n",
    "        'val_loss': [], 'val_rmse': [], 'val_coverage': []\n",
    "    }\n",
    "    \n",
    "    model.train()\n",
    "    pbar = trange(epochs, desc=\"Training MMNN_BNN\")\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        # Current beta for KL annealing\n",
    "        beta = beta_schedule(epoch)\n",
    "        \n",
    "        # Training phase\n",
    "        epoch_loss, epoch_likelihood, epoch_kl = 0.0, 0.0, 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute ELBO loss\n",
    "            loss, likelihood, kl_div = model.elbo_loss(\n",
    "                batch_x, batch_y, n_samples=n_mc_samples, beta=beta\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_likelihood += likelihood.item()\n",
    "            epoch_kl += kl_div.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average training metrics\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_likelihood = epoch_likelihood / num_batches\n",
    "        avg_kl = epoch_kl / num_batches\n",
    "        \n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['train_likelihood'].append(avg_likelihood)\n",
    "        history['train_kl'].append(avg_kl)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_metrics = {}\n",
    "        if val_loader is not None:\n",
    "            val_metrics = evaluate_mmnn_bnn(model, val_loader, device)\n",
    "            history['val_loss'].append(val_metrics['loss'])\n",
    "            history['val_rmse'].append(val_metrics['rmse'])\n",
    "            history['val_coverage'].append(val_metrics['coverage_95'])\n",
    "            \n",
    "            scheduler.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar_dict = {\n",
    "            'loss': f\"{avg_loss:.3e}\",\n",
    "            'likelihood': f\"{avg_likelihood:.3e}\", \n",
    "            'kl': f\"{avg_kl:.3e}\",\n",
    "            'beta': f\"{beta:.3f}\"\n",
    "        }\n",
    "        if val_metrics:\n",
    "            pbar_dict.update({\n",
    "                'val_rmse': f\"{val_metrics['rmse']:.3e}\",\n",
    "                'val_cov': f\"{val_metrics['coverage_95']:.2f}\"\n",
    "            })\n",
    "        \n",
    "        pbar.set_postfix(pbar_dict)\n",
    "\n",
    "        if epoch %100 == 0:\n",
    "\n",
    "\n",
    "            x_test = torch.linspace(-1.5, 1.5, 80).unsqueeze(1)\n",
    "            preds = model._forward_mc(x_test, 100)\n",
    "            pred_mean = preds.mean(dim=0)\n",
    "            pred_std = preds.std(dim=0)\n",
    "            lower = pred_mean - 1.96 * pred_std\n",
    "            upper = pred_mean + 1.96 * pred_std\n",
    "            plt.plot(x_test.flatten(), func(x_test.flatten()), 'r-', label='True')\n",
    "            plt.plot(x_test.flatten(), pred_mean.flatten().detach().cpu().numpy(), 'b-', label='Prediction')\n",
    "            plt.fill_between(x_test.flatten(), lower.flatten().detach().cpu().numpy(), upper.flatten().detach().cpu().numpy(), \n",
    "                     alpha=0.3, label='95% CI')\n",
    "            plt.show()\n",
    "        \n",
    "        # Early stopping on KL explosion\n",
    "        if avg_kl > 1e6:\n",
    "            print(f\"Training stopped at epoch {epoch}: KL divergence exploded\")\n",
    "            break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "df329cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mmnn_bnn(model, data_loader, device, n_mc_samples=50):\n",
    "    \"\"\"\n",
    "    Evaluate MMNN_BNN with uncertainty quantification\n",
    "    \n",
    "    Args:\n",
    "        model: Trained MMNN_BNN\n",
    "        data_loader: Data loader for evaluation\n",
    "        device: Computing device\n",
    "        n_mc_samples: MC samples for uncertainty estimation\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_targets = []\n",
    "    all_means = []\n",
    "    all_stds = []\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Monte Carlo sampling for uncertainty\n",
    "            mc_samples = model._forward_mc(batch_x, n_mc_samples)  # [n_samples, batch, output]\n",
    "            \n",
    "            # Compute statistics\n",
    "            pred_mean = mc_samples.mean(dim=0)  # [batch, output]\n",
    "            pred_std = mc_samples.std(dim=0)    # [batch, output]\n",
    "            \n",
    "            # Accumulate for metrics\n",
    "            all_targets.append(batch_y.cpu())\n",
    "            all_means.append(pred_mean.cpu())\n",
    "            all_stds.append(pred_std.cpu())\n",
    "            \n",
    "            # Loss computation\n",
    "            loss, _, _ = model.elbo_loss(batch_x, batch_y, n_samples=1, beta=1.0)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    # Concatenate all predictions\n",
    "    targets = torch.cat(all_targets, dim=0)  # [total_samples, output_dim]\n",
    "    means = torch.cat(all_means, dim=0)\n",
    "    stds = torch.cat(all_stds, dim=0)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # Predictive performance\n",
    "    metrics['loss'] = total_loss / num_batches\n",
    "    metrics['rmse'] = torch.sqrt(torch.mean((targets - means)**2)).item()\n",
    "    metrics['mae'] = torch.mean(torch.abs(targets - means)).item()\n",
    "    \n",
    "    # Uncertainty calibration\n",
    "    errors = torch.abs(targets - means)\n",
    "    \n",
    "    # Coverage probabilities (assuming Gaussian predictive distribution)\n",
    "    z_scores = errors / (stds + 1e-8)  # Avoid division by zero\n",
    "    \n",
    "    # 68% coverage (±1σ)\n",
    "    metrics['coverage_68'] = (z_scores <= 1.0).float().mean().item()\n",
    "    \n",
    "    # 95% coverage (±1.96σ)  \n",
    "    metrics['coverage_95'] = (z_scores <= 1.96).float().mean().item()\n",
    "    \n",
    "    # Average prediction interval width\n",
    "    metrics['avg_uncertainty'] = stds.mean().item()\n",
    "    \n",
    "    # Calibration: correlation between predicted uncertainty and actual error\n",
    "    if stds.numel() > 1:\n",
    "        correlation = torch.corrcoef(torch.stack([stds.flatten(), errors.flatten()]))[0,1]\n",
    "        metrics['uncertainty_correlation'] = correlation.item() if not torch.isnan(correlation) else 0.0\n",
    "    else:\n",
    "        metrics['uncertainty_correlation'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "105dc38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_uncertainty(model, x, n_samples=100, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Make predictions with uncertainty intervals\n",
    "    \n",
    "    Args:\n",
    "        model: Trained MMNN_BNN\n",
    "        x: Input tensor\n",
    "        n_samples: MC samples for uncertainty\n",
    "        confidence_level: Confidence level for intervals\n",
    "        \n",
    "    Returns:\n",
    "        mean: Predictive mean\n",
    "        std: Predictive standard deviation  \n",
    "        lower: Lower confidence bound\n",
    "        upper: Upper confidence bound\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Monte Carlo sampling\n",
    "        samples = model._forward_mc(x, n_samples)  # [n_samples, batch, output]\n",
    "        \n",
    "        # Compute statistics\n",
    "        mean = samples.mean(dim=0)\n",
    "        std = samples.std(dim=0)\n",
    "        \n",
    "        # Confidence intervals (assuming Gaussian)\n",
    "        from scipy.stats import norm\n",
    "        z_score = norm.ppf((1 + confidence_level) / 2)\n",
    "        \n",
    "        lower = mean - z_score * std\n",
    "        upper = mean + z_score * std\n",
    "        \n",
    "    return mean, std, lower, upper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7d436404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def test_mmnn_bnn_integration(model):\n",
    "    # Generate data\n",
    "    torch.manual_seed(42)\n",
    "    x_train = torch.linspace(-1, 1, 1000).unsqueeze(1)\n",
    "    \n",
    "    # y_train = func(2 * np.pi * x_train) + 0.00 * torch.randn_like(x_train)\n",
    "    y_train = func(x_train) + 0.0 * torch.randn_like(x_train)\n",
    "\n",
    "    plt.scatter(x_train, y_train, alpha=0.6, s=20, label='Train')\n",
    "    plt.show()\n",
    "    \n",
    "    x_val = torch.linspace(-1.2, 1.2, 50).unsqueeze(1)\n",
    "    # y_val = func(2 * np.pi * x_val) + 0.000 * torch.randn_like(x_val)\n",
    "    y_val = func(x_val) + 0.0 * torch.randn_like(x_val)\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=100, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=100)\n",
    "    \n",
    "    # Create model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    print(\"Testing MMNN_BNN training...\")\n",
    "    history = train_mmnn_bnn(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=2500,\n",
    "        lr=1e-4,\n",
    "        beta_schedule=lambda epoch: min(1.0, epoch / 1000),\n",
    "        n_mc_samples=1\n",
    "    )\n",
    "    \n",
    "    # Test prediction\n",
    "    print(\"Testing uncertainty prediction...\")\n",
    "    x_test = torch.linspace(-1.5, 1.5, 80).unsqueeze(1)\n",
    "    \n",
    "    mean, std, lower, upper = predict_with_uncertainty(\n",
    "        model, x_test, n_samples=30, confidence_level=0.95\n",
    "    )\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    if history['val_loss']:\n",
    "        plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Progress')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(x_train, y_train, alpha=0.6, s=10, label='Train')\n",
    "    # plt.plot(x_test, func(2 * np.pi * x_test), 'r-', label='True')\n",
    "    plt.plot(x_test, func(x_test), 'r-', label='True')\n",
    "    plt.plot(x_test, mean, 'b-', label='Prediction')\n",
    "    plt.fill_between(x_test.flatten(), lower.flatten(), upper.flatten(), \n",
    "                     alpha=0.3, label='95% CI')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.title('Predictions with Uncertainty')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check if working\n",
    "    final_loss = history['train_loss'][-1]\n",
    "    coverage = history['val_coverage'][-1] if history['val_coverage'] else 0.5\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Final loss: {final_loss:.3f}\")\n",
    "    print(f\"Val coverage: {coverage:.3f}\")\n",
    "    print(f\"Expected coverage: ~0.95\")\n",
    "    \n",
    "    success = (final_loss < 1000) and (0.8 < coverage < 1.0)\n",
    "    print(f\"Test {'PASSED' if success else 'FAILED'}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3603fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MMNN_BNN(\n",
    "        ranks=[1]+[36]*5+[1],  # Example ranks\n",
    "        widths=[666]*6, \n",
    "        prior_var=0.05\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, history = test_mmnn_bnn_integration(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beda7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "print(\"Testing uncertainty prediction...\")\n",
    "x_test = torch.linspace(-1.5, 1.5, 2000).unsqueeze(1)\n",
    "\n",
    "mean, std, lower, upper = predict_with_uncertainty(\n",
    "    model, x_test, n_samples=100, confidence_level=0.95\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "\n",
    "plt.plot(x_test, func(x_test), 'r-', label='True')\n",
    "plt.plot(x_test, mean, 'b-', label='Prediction')\n",
    "plt.fill_between(x_test.flatten(), lower.flatten(), upper.flatten(), \n",
    "                    alpha=0.3, label='95% CI')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Predictions with Uncertainty')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69ea31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
