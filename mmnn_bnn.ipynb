{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17256bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import math\n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('figures/images_style.mplstyle')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: torch.cos(20*torch.pi*torch.abs(x)**(1.4))+0.5*torch.cos(12*torch.pi*torch.abs(x)**(1.6))\n",
    "# func = lambda x: torch.sin(5 * np.pi * x)  # Simpler function for testing\n",
    "# Quick integration test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinTu(nn.Module):\n",
    "    def __init__(self, s = -torch.pi):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_x_s = torch.max(x, self.s*torch.ones_like(x))\n",
    "        return torch.sin(max_x_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c206dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMNN_BNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 ranks=[1] + [16]*5 + [1], \n",
    "                 widths=[366]*6,\n",
    "                 device=\"cpu\",\n",
    "                 prior_var=0.1,\n",
    "                 ResNet=False,\n",
    "                 variance_strategy=\"uniform\",\n",
    "                 n_bayesian_layers=None,\n",
    "                 activation_fn = SinTu):  # New parameter\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        MMNN_BNN: Bayesian Neural Network using MMNN structure with selective Bayesian layers\n",
    "        \n",
    "        Args:\n",
    "            ranks: Output dimensions per layer [input_dim, ..., output_dim]\n",
    "            widths: Width (number of basis functions) per layer\n",
    "            device: Computing device\n",
    "            prior_var: Prior variance for BNN layers\n",
    "            ResNet: Whether to use residual connections\n",
    "            variance_strategy: \"uniform\", \"decreasing\", \"increasing\", or \"xavier\"\n",
    "            n_bayesian_layers: Number of W,b layers to make Bayesian (from last to first)\n",
    "                              If None, all W,b layers are Bayesian (original behavior)\n",
    "                              If 0, no layers are Bayesian (deterministic MMNN)\n",
    "                              If k, the last k W,b layers are Bayesian\n",
    "        \"\"\"\n",
    "        self.ranks = ranks\n",
    "        self.widths = widths\n",
    "        self.ResNet = ResNet\n",
    "        self.depth = len(widths)\n",
    "        self.device = device\n",
    "        self.prior_var = prior_var\n",
    "        self.variance_strategy = variance_strategy\n",
    "        self.activation_fn = activation_fn()  # Activation function   \n",
    "        # Determine which layers should be Bayesian\n",
    "        self.n_bayesian_layers = n_bayesian_layers\n",
    "        if self.n_bayesian_layers is None:\n",
    "            self.n_bayesian_layers = self.depth  # All layers Bayesian (original behavior)\n",
    "        \n",
    "        # Validate n_bayesian_layers\n",
    "        if self.n_bayesian_layers < 0 or self.n_bayesian_layers > self.depth:\n",
    "            raise ValueError(f\"n_bayesian_layers must be between 0 and {self.depth}, got {self.n_bayesian_layers}\")\n",
    "        \n",
    "        print(f\"MMNN_BNN Configuration:\")\n",
    "        print(f\"  Total W,b layers: {self.depth}\")\n",
    "        print(f\"  Bayesian W,b layers: {self.n_bayesian_layers}\")\n",
    "        if self.n_bayesian_layers > 0:\n",
    "            bayesian_layer_indices = list(range(self.depth - self.n_bayesian_layers, self.depth))\n",
    "            print(f\"  Bayesian layer indices: {bayesian_layer_indices}\")\n",
    "        \n",
    "        # Build layer sizes: [rank1, width1, rank2, width2, rank3, ..., rankn-1,withn-1, rank n]\n",
    "        fc_sizes = [ranks[0]]\n",
    "        for j in range(self.depth):\n",
    "            fc_sizes += [widths[j], ranks[j+1]]\n",
    "        \n",
    "        # Initialize layers\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.bnn_params = nn.ParameterDict()  # BNN variance parameters\n",
    "        \n",
    "        for j in range(len(fc_sizes)-1):\n",
    "            fc = nn.Linear(fc_sizes[j], fc_sizes[j+1], device=device)\n",
    "            self.fcs.append(fc)\n",
    "            \n",
    "            # For W,b layers (even indices): Check if this layer should be Bayesian\n",
    "            if j % 2 == 0:\n",
    "                layer_idx = j // 2  # True layer index (0, 1, 2, ...)\n",
    "                \n",
    "                # Determine if this layer should be Bayesian\n",
    "                # Bayesian layers start from the end: last n_bayesian_layers\n",
    "                is_bayesian = layer_idx >= (self.depth - self.n_bayesian_layers)\n",
    "\n",
    "                # Freeze the mean parameters (W, b)\n",
    "                fc.weight.requires_grad = False\n",
    "                fc.bias.requires_grad = False\n",
    "                \n",
    "                if is_bayesian:\n",
    "                    print(f\"  Making layer {layer_idx} (linear layer {j}) Bayesian\")\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # Store fixed means\n",
    "                    self.register_buffer(f'W_mean_{j}', fc.weight.clone())\n",
    "                    self.register_buffer(f'b_mean_{j}', fc.bias.clone())\n",
    "                    \n",
    "                    # Calculate layer-specific prior variance\n",
    "                    n_in = fc_sizes[j]\n",
    "                    n_out = fc_sizes[j+1]\n",
    "                    \n",
    "                    weight_prior_var, bias_prior_var = self.get_layer_prior_var(\n",
    "                        layer_idx, n_in, n_out, self.prior_var\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"    W_var={weight_prior_var:.6f}, b_var={bias_prior_var:.6f}\")\n",
    "                    \n",
    "                    # Initialize variance parameters (log-parameterization for stability)\n",
    "                    self.bnn_params[f'W_log_var_{j}'] = nn.Parameter(\n",
    "                        torch.full_like(fc.weight, math.log(weight_prior_var))\n",
    "                    ).to(device)\n",
    "                    self.bnn_params[f'b_log_var_{j}'] = nn.Parameter(\n",
    "                        torch.full_like(fc.bias, math.log(bias_prior_var))\n",
    "                    ).to(device)\n",
    "                    \n",
    "                    # Mark this layer as Bayesian for later reference\n",
    "                    self.register_buffer(f'is_bayesian_{j}', torch.tensor(True))\n",
    "                else:\n",
    "                    print(f\"  Keeping layer {layer_idx} (linear layer {j}) deterministic\")\n",
    "                    # Mark this layer as deterministic\n",
    "                    self.register_buffer(f'is_bayesian_{j}', torch.tensor(False))\n",
    "\n",
    "    def get_layer_prior_var(self, layer_idx, n_in, n_out, base_var=0.1):\n",
    "        \"\"\"\n",
    "        Calculate layer-specific prior variance with multiple strategies\n",
    "        \n",
    "        Args:\n",
    "            layer_idx: True MMNN layer index (0, 1, 2, ...)\n",
    "            n_in: Input dimension\n",
    "            n_out: Output dimension  \n",
    "            base_var: Base variance scale\n",
    "        \n",
    "        Returns:\n",
    "            weight_prior_var, bias_prior_var\n",
    "        \"\"\"\n",
    "        if self.variance_strategy == \"uniform\":\n",
    "            # Same variance for all layers\n",
    "            weight_var = base_var / n_in  # Standard scaling\n",
    "            bias_var = base_var#/n_in\n",
    "            \n",
    "        elif self.variance_strategy == \"xavier\":\n",
    "            # Xavier/Glorot-style initialization for Bayesian setting\n",
    "            weight_var = 2.0*base_var / (n_in + n_out)\n",
    "            bias_var = 1.0 *base_var#/ n_out\n",
    "            \n",
    "        elif self.variance_strategy == \"decreasing\":\n",
    "            # Earlier layers have higher variance (more uncertainty in basis functions)\n",
    "            depth_factor = (self.depth - layer_idx) / self.depth\n",
    "            weight_var = base_var * depth_factor / n_in\n",
    "            bias_var = base_var * depth_factor#/n_in\n",
    "            \n",
    "        elif self.variance_strategy == \"increasing\":\n",
    "            # Later layers have higher variance (more uncertainty in combinations)\n",
    "            depth_factor = (layer_idx + 1) / self.depth\n",
    "            weight_var = base_var * depth_factor / n_in\n",
    "            bias_var = base_var * depth_factor#/n_in\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown variance strategy: {self.variance_strategy}\")\n",
    "        \n",
    "        return weight_var, bias_var\n",
    "\n",
    "    def is_layer_bayesian(self, layer_linear_idx):\n",
    "        \"\"\"Check if a linear layer index is Bayesian\"\"\"\n",
    "        return getattr(self, f'is_bayesian_{layer_linear_idx}').item()\n",
    "\n",
    "    def sample_bnn_params(self):\n",
    "        \"\"\"Sample W,b parameters from variational posterior (only for Bayesian layers)\"\"\"\n",
    "        sampled_params = {}\n",
    "        \n",
    "        for name, param in self.bnn_params.items():\n",
    "            if 'W_log_var' in name:\n",
    "                layer_idx = name.split('_')[-1]\n",
    "                \n",
    "                # Get mean and variance\n",
    "                W_mean = getattr(self, f'W_mean_{layer_idx}')\n",
    "                W_var = torch.exp(param)  # Convert from log-variance\n",
    "                \n",
    "                # Sample using reparameterization trick\n",
    "                eps = torch.randn_like(W_mean, device=W_mean.device)\n",
    "                W_sample = W_mean + torch.sqrt(W_var) * eps\n",
    "                sampled_params[f'W_{layer_idx}'] = W_sample\n",
    "                \n",
    "            elif 'b_log_var' in name:\n",
    "                layer_idx = name.split('_')[-1]\n",
    "                \n",
    "                b_mean = getattr(self, f'b_mean_{layer_idx}')\n",
    "                b_var = torch.exp(param)\n",
    "                \n",
    "                eps = torch.randn_like(b_mean, device=b_mean.device)\n",
    "                b_sample = b_mean + torch.sqrt(b_var) * eps\n",
    "                sampled_params[f'b_{layer_idx}'] = b_sample\n",
    "        \n",
    "        return sampled_params\n",
    "    \n",
    "    def forward(self, x, n_samples=1):\n",
    "        \"\"\"\n",
    "        Forward pass with Monte Carlo sampling\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            n_samples: Number of MC samples for inference\n",
    "        \n",
    "        Returns:\n",
    "            output: Mean prediction (if n_samples=1) or samples\n",
    "        \"\"\"\n",
    "        if n_samples == 1:\n",
    "            return self._forward_single(x)\n",
    "        else:\n",
    "            return self._forward_mc(x, n_samples)\n",
    "    \n",
    "    def _forward_single(self, x):\n",
    "        \"\"\"Single forward pass for training\"\"\"\n",
    "        sampled_params = self.sample_bnn_params()\n",
    "        \n",
    "        for j in range(self.depth):\n",
    "            if self.ResNet and 0 < j < self.depth-1:\n",
    "                x_id = x.clone()\n",
    "            \n",
    "            # W,b layer - check if Bayesian or deterministic\n",
    "            layer_idx = 2 * j\n",
    "            \n",
    "            if self.is_layer_bayesian(layer_idx) and f'W_{layer_idx}' in sampled_params:\n",
    "                # Use Bayesian sampled parameters\n",
    "                W = sampled_params[f'W_{layer_idx}']\n",
    "                b = sampled_params[f'b_{layer_idx}']\n",
    "                x = F.linear(x, W, b)\n",
    "            else:\n",
    "                # Use deterministic parameters\n",
    "                x = self.fcs[layer_idx](x)\n",
    "            \n",
    "            # x = torch.relu(x)\n",
    "            x = self.activation_fn(x)  # Use the specified activation function\n",
    "            \n",
    "            # Deterministic A layer (always deterministic)\n",
    "            x = self.fcs[layer_idx + 1](x)\n",
    "            \n",
    "            # ResNet connection\n",
    "            if self.ResNet and 0 < j < self.depth-1:\n",
    "                n = min(x.shape[1], x_id.shape[1])\n",
    "                x[:, :n] = x[:, :n] + x_id[:, :n]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _forward_mc(self, x, n_samples):\n",
    "        \"\"\"Multiple forward passes for uncertainty estimation\"\"\"\n",
    "        outputs = []\n",
    "        for _ in range(n_samples):\n",
    "            outputs.append(self._forward_single(x))\n",
    "        return torch.stack(outputs, dim=0)  # [n_samples, batch_size, output_dim]\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        \"\"\"Compute KL divergence between posterior and prior (only for Bayesian layers)\"\"\"\n",
    "        kl_div = 0.0\n",
    "        \n",
    "        for name, log_var_param in self.bnn_params.items():\n",
    "            if 'W_log_var' in name:\n",
    "                layer_idx_str = name.split('_')[-1]\n",
    "                layer_idx_int = int(layer_idx_str)\n",
    "                layer_idx = layer_idx_int // 2  # Convert to true layer index\n",
    "                \n",
    "                # Get layer dimensions\n",
    "                n_in = self.fcs[layer_idx_int].in_features\n",
    "                n_out = self.fcs[layer_idx_int].out_features\n",
    "                \n",
    "                # Get prior variance for this layer\n",
    "                weight_prior_var, _ = self.get_layer_prior_var(layer_idx, n_in, n_out, self.prior_var)\n",
    "                prior_log_var = math.log(weight_prior_var)\n",
    "                \n",
    "            elif 'b_log_var' in name:\n",
    "                layer_idx_str = name.split('_')[-1]\n",
    "                layer_idx_int = int(layer_idx_str)\n",
    "                layer_idx = layer_idx_int // 2\n",
    "                \n",
    "                n_in = self.fcs[layer_idx_int].in_features\n",
    "                n_out = self.fcs[layer_idx_int].out_features\n",
    "                \n",
    "                _, bias_prior_var = self.get_layer_prior_var(layer_idx, n_in, n_out, self.prior_var)\n",
    "                prior_log_var = math.log(bias_prior_var)\n",
    "            \n",
    "            # KL[N(μ, σ²) || N(μ, σ_p²)] = 1/2 * [log(σ_p²/σ²) + σ²/σ_p² - 1]\n",
    "            posterior_var = torch.exp(log_var_param)\n",
    "            prior_var = math.exp(prior_log_var)\n",
    "            \n",
    "            kl_layer = 0.5 * (\n",
    "                prior_log_var - log_var_param + \n",
    "                posterior_var / prior_var - 1.0\n",
    "            )\n",
    "            kl_div += kl_layer.sum()\n",
    "        \n",
    "        return kl_div\n",
    "    \n",
    "    def elbo_loss(self, x, y, n_samples=1, beta=1.0):\n",
    "        \"\"\"\n",
    "        Compute ELBO loss for training\n",
    "        \n",
    "        Args:\n",
    "            x, y: Input and target\n",
    "            n_samples: MC samples for likelihood estimation\n",
    "            beta: KL weighting (β-VAE style)\n",
    "        \"\"\"\n",
    "        # Likelihood term: E_q[log p(y|x,θ)]\n",
    "        if n_samples == 1:\n",
    "            y_pred = self._forward_single(x)\n",
    "            log_likelihood = -0.5 * F.mse_loss(y_pred, y, reduction='sum')\n",
    "        else:\n",
    "            y_samples = self._forward_mc(x, n_samples)  # [n_samples, batch, output]\n",
    "            log_likelihood = 0.0\n",
    "            for i in range(n_samples):\n",
    "                log_likelihood += -0.5 * F.mse_loss(y_samples[i], y, reduction='sum')\n",
    "            log_likelihood /= n_samples\n",
    "        \n",
    "        # KL term: KL[q(θ)||p(θ)] (only for Bayesian layers)\n",
    "        kl_div = self.kl_divergence()\n",
    "        \n",
    "        # ELBO = E[log p(y|x,θ)] - β * KL[q(θ)||p(θ)]\n",
    "        elbo = log_likelihood - beta * kl_div\n",
    "        \n",
    "        # Return negative ELBO for minimization\n",
    "        return -elbo, log_likelihood, kl_div\n",
    "    \n",
    "    def l2_loss(self, x, y):\n",
    "        \"\"\"Compute L2 loss for regression tasks\"\"\"\n",
    "        y_pred = self._forward_single(x)\n",
    "        return F.mse_loss(y_pred, y, reduction='mean')\n",
    "\n",
    "    def predict_with_uncertainty(self, x, n_samples=100):\n",
    "        \"\"\"Get predictive mean and uncertainty\"\"\"\n",
    "        with torch.no_grad():\n",
    "            samples = self._forward_mc(x, n_samples)  # [n_samples, batch, output]\n",
    "            \n",
    "            pred_mean = samples.mean(dim=0)\n",
    "            pred_var = samples.var(dim=0)\n",
    "            epistemic_uncertainty = pred_var.mean(dim=-1)  # Average over output dims\n",
    "            \n",
    "            return pred_mean, pred_var, epistemic_uncertainty\n",
    "    \n",
    "    def get_bayesian_layer_info(self):\n",
    "        \"\"\"Get information about which layers are Bayesian\"\"\"\n",
    "        info = {\n",
    "            'total_layers': self.depth,\n",
    "            'bayesian_layers': self.n_bayesian_layers,\n",
    "            'bayesian_layer_indices': [],\n",
    "            'deterministic_layer_indices': []\n",
    "        }\n",
    "        \n",
    "        for j in range(0, len(self.fcs), 2):  # Only W,b layers (even indices)\n",
    "            layer_idx = j // 2\n",
    "            if self.is_layer_bayesian(j):\n",
    "                info['bayesian_layer_indices'].append(layer_idx)\n",
    "            else:\n",
    "                info['deterministic_layer_indices'].append(layer_idx)\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total, trainable, and Bayesian parameters\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        bayesian_params = sum(p.numel() for p in self.bnn_params.values())\n",
    "        \n",
    "        # Count deterministic trainable parameters (A, c layers)\n",
    "        deterministic_trainable = 0\n",
    "        for j in range(1, len(self.fcs), 2):  # A layers (odd indices)\n",
    "            deterministic_trainable += sum(p.numel() for p in self.fcs[j].parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'bayesian_parameters': bayesian_params,\n",
    "            'deterministic_trainable_parameters': deterministic_trainable,\n",
    "            'frozen_parameters': total_params - trainable_params\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703cb65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mmnn_bnn(model, data_loader, device, n_mc_samples=50):\n",
    "    \"\"\"\n",
    "    Evaluate MMNN_BNN with uncertainty quantification\n",
    "    \n",
    "    Args:\n",
    "        model: Trained MMNN_BNN\n",
    "        data_loader: Data loader for evaluation\n",
    "        device: Computing device\n",
    "        n_mc_samples: MC samples for uncertainty estimation\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_targets = []\n",
    "    all_means = []\n",
    "    all_stds = []\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Monte Carlo sampling for uncertainty\n",
    "            mc_samples = model._forward_mc(batch_x, n_mc_samples)  # [n_samples, batch, output]\n",
    "            \n",
    "            # Compute statistics\n",
    "            pred_mean = mc_samples.mean(dim=0)  # [batch, output]\n",
    "            pred_std = mc_samples.std(dim=0)    # [batch, output]\n",
    "            \n",
    "            # Accumulate for metrics\n",
    "            all_targets.append(batch_y.cpu())\n",
    "            all_means.append(pred_mean.cpu())\n",
    "            all_stds.append(pred_std.cpu())\n",
    "            \n",
    "            # Loss computation\n",
    "            loss, _, _ = model.elbo_loss(batch_x, batch_y, n_samples=1, beta=1.0)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    # Concatenate all predictions\n",
    "    targets = torch.cat(all_targets, dim=0)  # [total_samples, output_dim]\n",
    "    means = torch.cat(all_means, dim=0)\n",
    "    stds = torch.cat(all_stds, dim=0)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # Predictive performance\n",
    "    metrics['loss'] = total_loss / num_batches\n",
    "    metrics['rmse'] = torch.sqrt(torch.mean((targets - means)**2)).item()\n",
    "    metrics['mae'] = torch.mean(torch.abs(targets - means)).item()\n",
    "    \n",
    "    # Uncertainty calibration\n",
    "    errors = torch.abs(targets - means)\n",
    "    \n",
    "    # Coverage probabilities (assuming Gaussian predictive distribution)\n",
    "    z_scores = errors / (stds + 1e-8)  # Avoid division by zero\n",
    "    \n",
    "    # 68% coverage (±1σ)\n",
    "    metrics['coverage_68'] = (z_scores <= 1.0).float().mean().item()\n",
    "    \n",
    "    # 95% coverage (±1.96σ)  \n",
    "    metrics['coverage_95'] = (z_scores <= 1.96).float().mean().item()\n",
    "    \n",
    "    # Average prediction interval width\n",
    "    metrics['avg_uncertainty'] = stds.mean().item()\n",
    "    \n",
    "    # Calibration: correlation between predicted uncertainty and actual error\n",
    "    if stds.numel() > 1:\n",
    "        correlation = torch.corrcoef(torch.stack([stds.flatten(), errors.flatten()]))[0,1]\n",
    "        metrics['uncertainty_correlation'] = correlation.item() if not torch.isnan(correlation) else 0.0\n",
    "    else:\n",
    "        metrics['uncertainty_correlation'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mmnn_bnn(model, train_loader, val_loader=None, \n",
    "                   epochs=1000, lr=1e-3, beta_schedule=None, \n",
    "                   n_mc_samples=1, device='cpu'):\n",
    "    \"\"\"\n",
    "    Training scheme for MMNN_BNN\n",
    "    \n",
    "    Args:\n",
    "        model: MMNN_BNN instance\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader (optional)\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        beta_schedule: Function for KL annealing (epoch -> beta)\n",
    "        n_mc_samples: MC samples during training (1 for efficiency)\n",
    "        device: Computing device\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optimizer - separate learning rates for different parameter types\n",
    "    bnn_params = list(model.bnn_params.parameters())\n",
    "    bnn_param_ids = {id(p) for p in bnn_params}\n",
    "    mlp_params = [p for p in model.parameters() if p.requires_grad and id(p) not in bnn_param_ids]\n",
    "    \n",
    "    optimizer = optim.Adam([\n",
    "        {'params': bnn_params, 'lr': lr//5, 'name': 'bnn'},\n",
    "        {'params': mlp_params, 'lr': lr , 'name': 'mlp'}\n",
    "    ])\n",
    "    \n",
    "    # scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)  \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=600, gamma=0.9)\n",
    "    \n",
    "    # Default beta schedule: gradual annealing from 0 to 1\n",
    "    if beta_schedule is None:\n",
    "        beta_schedule = lambda epoch: min(1.0, epoch / (epochs /200))\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_likelihood': [], 'train_kl': [],\n",
    "        'val_loss': [], 'val_rmse': [], 'val_coverage': []\n",
    "    }\n",
    "    \n",
    "    model.train()\n",
    "    pbar = trange(epochs, desc=\"Training MMNN_BNN\")\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        # Current beta for KL annealing\n",
    "        beta = beta_schedule(epoch)\n",
    "        \n",
    "        # Training phase\n",
    "        epoch_loss, epoch_likelihood, epoch_kl = 0.0, 0.0, 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if model.n_bayesian_layers > 0:\n",
    "                # Compute ELBO loss\n",
    "                loss, likelihood, kl_div = model.elbo_loss(\n",
    "                    batch_x, batch_y, n_samples=n_mc_samples, beta=beta\n",
    "                )\n",
    "            else:\n",
    "                # Compute L2 loss for deterministic MMNN\n",
    "                loss = model.l2_loss(batch_x, batch_y)\n",
    "                likelihood = torch.tensor(0.0, device=device)  # No likelihood term\n",
    "                kl_div = torch.tensor(0.0, device=device)  # No KL divergence\n",
    "                # likelihood = -loss\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            epoch_likelihood += likelihood.item()\n",
    "            \n",
    "            epoch_kl += kl_div.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average training metrics\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_likelihood = epoch_likelihood / num_batches\n",
    "        avg_kl = epoch_kl / num_batches\n",
    "        \n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['train_likelihood'].append(avg_likelihood)\n",
    "        history['train_kl'].append(avg_kl)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_metrics = {}\n",
    "        if val_loader is not None:\n",
    "            val_metrics = evaluate_mmnn_bnn(model, val_loader, device)\n",
    "            history['val_loss'].append(val_metrics['loss'])\n",
    "            history['val_rmse'].append(val_metrics['rmse'])\n",
    "            history['val_coverage'].append(val_metrics['coverage_95'])\n",
    "            \n",
    "            scheduler.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar_dict = {\n",
    "            'loss': f\"{avg_loss:.3e}\",\n",
    "            'likelihood': f\"{avg_likelihood:.3e}\", \n",
    "            'kl': f\"{avg_kl:.3e}\",\n",
    "            'beta': f\"{beta:.3f}\"\n",
    "        }\n",
    "        if val_metrics:\n",
    "            pbar_dict.update({\n",
    "                'val_rmse': f\"{val_metrics['rmse']:.3e}\",\n",
    "                'val_cov': f\"{val_metrics['coverage_95']:.2f}\"\n",
    "            })\n",
    "        \n",
    "        pbar.set_postfix(pbar_dict)\n",
    "\n",
    "        if epoch %500 == 0:\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x_test = torch.linspace(-1., 1., 1000).unsqueeze(1).to(device)\n",
    "                preds = model._forward_mc(x_test, 100)\n",
    "                pred_mean = preds.mean(dim=0)\n",
    "                pred_std = preds.std(dim=0)\n",
    "                lower = pred_mean - 1.96 * pred_std\n",
    "                upper = pred_mean + 1.96 * pred_std\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                plt.plot(x_test.cpu().flatten(), func(x_test.cpu().flatten()),'r-',linewidth = 0.5, label='True')\n",
    "                plt.plot(x_test.cpu().flatten(), pred_mean.flatten().detach().cpu().numpy(), 'b-', label='Prediction')\n",
    "                plt.fill_between(x_test.cpu().flatten(), lower.flatten().detach().cpu().numpy(), upper.flatten().detach().cpu().numpy(), \n",
    "                        alpha=0.3, label='95% CI')\n",
    "                plt.title(f\"Epoch {epoch} - ELBO: {avg_loss:.3e}, KL: {avg_kl:.3e}, Beta: {beta:.3f}\")\n",
    "                plt.show()\n",
    "        \n",
    "        # Early stopping on KL explosion\n",
    "        if avg_kl > 1e6:\n",
    "            print(f\"Training stopped at epoch {epoch}: KL divergence exploded\")\n",
    "            break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105dc38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_uncertainty(model, x, n_samples=100, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Make predictions with uncertainty intervals\n",
    "    \n",
    "    Args:\n",
    "        model: Trained MMNN_BNN\n",
    "        x: Input tensor\n",
    "        n_samples: MC samples for uncertainty\n",
    "        confidence_level: Confidence level for intervals\n",
    "        \n",
    "    Returns:\n",
    "        mean: Predictive mean\n",
    "        std: Predictive standard deviation  \n",
    "        lower: Lower confidence bound\n",
    "        upper: Upper confidence bound\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Monte Carlo sampling\n",
    "        samples = model._forward_mc(x, n_samples)  # [n_samples, batch, output]\n",
    "        \n",
    "        # Compute statistics\n",
    "        mean = samples.mean(dim=0)\n",
    "        std = samples.std(dim=0)\n",
    "        \n",
    "        # Confidence intervals (assuming Gaussian)\n",
    "        from scipy.stats import norm\n",
    "        z_score = norm.ppf((1 + confidence_level) / 2)\n",
    "        \n",
    "        lower = mean - z_score * std\n",
    "        upper = mean + z_score * std\n",
    "        \n",
    "    return mean, std, lower, upper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d436404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def test_mmnn_bnn_integration(model,n_epochs=10_000,lr = 1e-3,device  = 'cpu'):\n",
    "    # Generate data\n",
    "    torch.manual_seed(42)\n",
    "    x_train_no_noise = torch.cat([\n",
    "        torch.linspace(-1, -0.2, 400).unsqueeze(1),\n",
    "        torch.linspace(0.2, 1, 400).unsqueeze(1)\n",
    "    ]).to(device)\n",
    "    y_train_no_noise = func(x_train_no_noise) + 0.0 * torch.randn_like(x_train_no_noise)\n",
    "\n",
    "\n",
    "    x_train_noise = torch.linspace(-.2,.2, 200).unsqueeze(1).to(device)\n",
    "    y_train_noise = func(x_train_noise) + 0.15 * torch.randn_like(x_train_noise)    \n",
    "    \n",
    "    x_train = torch.cat([x_train_no_noise, x_train_noise], dim=0).to(device)\n",
    "    y_train = torch.cat([y_train_no_noise, y_train_noise], dim=0).to(device)\n",
    "\n",
    "    plt.scatter(x_train.cpu(), y_train.cpu(), alpha=0.6, s=20, label='Train')\n",
    "    plt.show()\n",
    "    \n",
    "    x_val = torch.linspace(-1.2, 1.2, 50).unsqueeze(1).to(device)\n",
    "    # y_val = func(2 * np.pi * x_val) + 0.000 * torch.randn_like(x_val)\n",
    "    y_val = func(x_val) + 0.0 * torch.randn_like(x_val)\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=100, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size=100)\n",
    "    \n",
    "    # Create model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    print(\"Testing MMNN_BNN training...\")\n",
    "    history = train_mmnn_bnn(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=n_epochs,\n",
    "        lr=lr,\n",
    "        beta_schedule=lambda epoch: min(1.0, epoch / 1000),\n",
    "        n_mc_samples=1,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Test prediction\n",
    "    print(\"Testing uncertainty prediction...\")\n",
    "    x_test = torch.linspace(-1, 1., 1000).unsqueeze(1).to(device)\n",
    "    \n",
    "    mean, std, lower, upper = predict_with_uncertainty(\n",
    "        model, x_test, n_samples=50, confidence_level=0.90\n",
    "    )\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    if history['val_loss']:\n",
    "        plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Progress')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(x_train.cpu(), y_train.cpu(), alpha=0.6, s=10, label='Train')\n",
    "    # plt.plot(x_test, func(2 * np.pi * x_test), 'r-', label='True')\n",
    "    plt.plot(x_test.cpu(), func(x_test).cpu(), 'r-',linewidth = 0.5, label='True')\n",
    "    plt.plot(x_test.cpu(), mean.detach().cpu(), 'b-', label='Prediction')\n",
    "    plt.fill_between(x_test.cpu().flatten(), lower.flatten().detach().cpu(), upper.flatten().detach().cpu(), \n",
    "                     alpha=0.3, label='95% CI')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.title('Predictions with Uncertainty')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check if working\n",
    "    final_loss = history['train_loss'][-1]\n",
    "    coverage = history['val_coverage'][-1] if history['val_coverage'] else 0.5\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Final loss: {final_loss:.3f}\")\n",
    "    print(f\"Val coverage: {coverage:.3f}\")\n",
    "    print(f\"Expected coverage: ~0.95\")\n",
    "    \n",
    "    success = (final_loss < 1000) and (0.8 < coverage < 1.0)\n",
    "    print(f\"Test {'PASSED' if success else 'FAILED'}\")\n",
    "    \n",
    "    return model, history,(x_train, y_train)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "ranks = [1]+[36]*(num_layers-1)+[1]\n",
    "widths = [2**8]*num_layers\n",
    "prior_var = .5\n",
    "\n",
    "\n",
    "model = MMNN_BNN(\n",
    "        ranks= ranks,  # Example ranks\n",
    "        widths=widths, \n",
    "        prior_var=prior_var,\n",
    "        device=device,\n",
    "        ResNet=False,\n",
    "        n_bayesian_layers=2,\n",
    "        variance_strategy=\"increasing\"  # or \"uniform\", \"increasing\", \"xavier\"\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, history, (x_train, y_train) = test_mmnn_bnn_integration(model,n_epochs=10000,lr = 1e-3,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beda7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "print(\"Testing uncertainty prediction...\")\n",
    "x_test = torch.linspace(-1.2, 1.2, 2000).unsqueeze(1).to(device)\n",
    "\n",
    "mean, std, lower, upper = predict_with_uncertainty(\n",
    "    model, x_test, n_samples=50, confidence_level=0.95\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "\n",
    "plt.plot(x_train.cpu(), y_train.cpu() ,'r*',markersize = 0.5, label='Training Data')\n",
    "plt.plot(x_test.cpu(), mean.detach().cpu(), 'b-', label='Prediction')\n",
    "plt.fill_between(x_test.cpu().flatten(), lower.flatten().detach().cpu(), upper.flatten().detach().cpu(), \n",
    "                    alpha=0.6, label='95% CI')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Predictions with Uncertainty')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(x_test.cpu(), func(x_test).cpu(), 'r*',markersize = 0.5, label='True')\n",
    "plt.plot(x_test.cpu(), model._forward_single(x_test).flatten().detach().cpu().numpy(), 'b-', label='Deterministic Prediction' )\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Deterministic Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e015743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
